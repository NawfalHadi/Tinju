{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line_and_calculate_gap(image, start_point, end_point):\n",
    "    if start_point and end_point:\n",
    "        x1, y1 = start_point.x * image.shape[1], start_point.y * image.shape[0]\n",
    "        x2, y2 = end_point.x * image.shape[1], end_point.y * image.shape[0]\n",
    "        \n",
    "        # Calculate normalized coordinates\n",
    "        gap_x = (x2 - x1) / image.shape[1]\n",
    "        gap_y = (y2 - y1) / image.shape[0]\n",
    "\n",
    "        # Draw line\n",
    "        x1, y1 = int(x1), int(y1)\n",
    "        x2, y2 = int(x2), int(y2)\n",
    "        cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "        gap_landmark = landmark_pb2.NormalizedLandmark()\n",
    "        gap_landmark.x = gap_x\n",
    "        gap_landmark.y = gap_y\n",
    "\n",
    "        return gap_landmark\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nawfa\\anaconda3\\envs\\cuda_12_1_clone\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\nawfa\\anaconda3\\envs\\cuda_12_1_clone\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.3.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\nawfa\\anaconda3\\envs\\cuda_12_1_clone\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.3.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svc&#x27;, SVC(probability=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svc&#x27;, SVC(probability=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(probability=True))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../Model/svm_mpgap.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluasi_model(file, pose_limit, poses):\n",
    "    cap = cv2.VideoCapture(f'Scenario/{file}')\n",
    "\n",
    "    counter = 0\n",
    "    recent_pose = None\n",
    "\n",
    "    pose_sequence = poses\n",
    "    pose_detected = []\n",
    "    condition = []\n",
    "\n",
    "    print(pose_limit)\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "        \n",
    "            # Recolor Feed\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False        \n",
    "        \n",
    "            # Make Detections\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            # Recolor image back to BGR for rendering\n",
    "            image.flags.writeable = True   \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "\n",
    "            # Get specific landmarks\n",
    "            nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "            wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_WRIST]\n",
    "            elbow_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_ELBOW]\n",
    "            wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "            elbow_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "        \n",
    "            # Create a custom landmark list to show\n",
    "            show_landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "            show_landmark_list.landmark.extend([nose, wrist_l, wrist_r, elbow_l, elbow_r])\n",
    "        \n",
    "            # Draw landmarks\n",
    "            for landmark in show_landmark_list.landmark:\n",
    "                x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "                cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
    "        \n",
    "            # Drawing line and calculating gap for left wrist\n",
    "            gap_nose_left = draw_line_and_calculate_gap(image, nose, wrist_l)\n",
    "\n",
    "            # Drawing line and calculating gap for right wrist\n",
    "            gap_nose_right = draw_line_and_calculate_gap(image, nose, wrist_r)\n",
    "\n",
    "            gap_hand_left = draw_line_and_calculate_gap(image, elbow_l, wrist_l)\n",
    "            gap_hand_right = draw_line_and_calculate_gap(image, elbow_r, wrist_r)\n",
    "\n",
    "\n",
    "            new_lm = landmark_pb2.NormalizedLandmarkList()\n",
    "            new_lm.landmark.extend([wrist_l, wrist_r, elbow_l, elbow_r, gap_nose_right, gap_nose_left])\n",
    "            # ===================================================== #\n",
    "\n",
    "            # Export coordinates\n",
    "            try:\n",
    "                # Extract Pose landmarks\n",
    "                pose = new_lm.landmark\n",
    "                pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "        \n",
    "                # Make Detections\n",
    "                X = pd.DataFrame([pose_row])\n",
    "\n",
    "                # Ignore scikit-learn warnings about feature names\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "                body_language_class = model.predict(X)[0]\n",
    "                body_language_prob = model.predict_proba(X)[0]\n",
    "            \n",
    "            \n",
    "                # Get status box\n",
    "                cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "                pose_percentage = round(body_language_prob[np.argmax(body_language_prob)],2)\n",
    "                if pose_percentage >= 0.70:\n",
    "                    # Display Class\n",
    "                    cv2.putText(image, 'CLASS'\n",
    "                                , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, body_language_class.split(' ')[0]\n",
    "                                , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "                    # Display Probability\n",
    "                    cv2.putText(image, 'PROB'\n",
    "                                , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "                                , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                                \n",
    "                    if body_language_class != recent_pose:\n",
    "                        recent_pose = body_language_class.split(' ')[0]\n",
    "                        \n",
    "                        condition.append(poses[0] == recent_pose)\n",
    "                        pose_detected.append(recent_pose)\n",
    "                        \n",
    "                        \n",
    "                        print(recent_pose, counter)\n",
    "                        counter += 1\n",
    "\n",
    "                    if counter == pose_limit:\n",
    "                        cap.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                        return pose_detected, condition\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction: {e}\")\n",
    "                        \n",
    "            cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ SCENARIO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "idle 0\n",
      "guard 1\n",
      "jab 2\n",
      "guard 3\n",
      "jab 4\n",
      "(['idle', 'guard', 'jab', 'guard', 'jab'], [False, False, True, False, True])\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')    \n",
    "\n",
    "with open('Scenario/scenario.csv', 'r') as csvfile, open(f'Tested/{timestamp}.csv', 'w', newline='') as outfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    data = list(reader)\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    for row in data:\n",
    "        scenario = row[0]\n",
    "        file = row[1]\n",
    "        pose = row[2:]\n",
    "\n",
    "        evaluated = evaluasi_model(file, len(pose)-1, pose)\n",
    "        print(evaluated)\n",
    "    \n",
    "        modified_row = row + ['detected'] + evaluated[0] + ['condition'] + evaluated[1]\n",
    "        writer.writerow(modified_row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12_1_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
